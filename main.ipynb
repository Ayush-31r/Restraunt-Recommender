{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb68f19b",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0263d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000f98d",
   "metadata": {},
   "source": [
    "LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1feb6e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes:\n",
      "- Customers: (34674, 8)\n",
      "- Locations: (59503, 5)\n",
      "- Orders: (135303, 26)\n",
      "- Vendors: (100, 59)\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "customers = pd.read_csv('Train/train_customers.csv')\n",
    "locations = pd.read_csv('Train/train_locations.csv')\n",
    "orders = pd.read_csv('Train/orders.csv', low_memory=False)\n",
    "vendors = pd.read_csv('Train/vendors.csv')\n",
    "\n",
    "# Load test data\n",
    "test_customers = pd.read_csv('Test/test_customers.csv')\n",
    "test_locations = pd.read_csv('Test/test_locations.csv')\n",
    "\n",
    "print(f\"Training data shapes:\")\n",
    "print(f\"- Customers: {customers.shape}\")\n",
    "print(f\"- Locations: {locations.shape}\")\n",
    "print(f\"- Orders: {orders.shape}\")\n",
    "print(f\"- Vendors: {vendors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2262b7b9",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0207ed9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns with >50.0% missing: ['dob']\n",
      "Dropping columns with >50.0% missing: ['promo_code', 'promo_code_discount_percentage', 'vendor_rating', 'delivery_time', 'driver_accepted_time', 'delivery_date']\n",
      "Dropping columns with >50.0% missing: ['sunday_from_time2', 'sunday_to_time2', 'monday_from_time2', 'monday_to_time2', 'tuesday_from_time2', 'tuesday_to_time2', 'wednesday_from_time2', 'wednesday_to_time2', 'thursday_from_time2', 'thursday_to_time2', 'friday_from_time2', 'friday_to_time2', 'saturday_from_time2', 'saturday_to_time2']\n",
      "Dropping columns with >50.0% missing: ['dob']\n"
     ]
    }
   ],
   "source": [
    "def clean_dataframe(df, threshold=0.5):\n",
    "    # Drop columns with >50% missing values\n",
    "    cols_to_drop = df.columns[df.isnull().mean() > threshold]\n",
    "    if len(cols_to_drop) > 0:\n",
    "        print(f\"Dropping columns with >{threshold*100}% missing: {list(cols_to_drop)}\")\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Fill numeric columns with median\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # Fill categorical columns with mode or 'unknown'\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if len(df[col].mode()) > 0:\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            else:\n",
    "                df[col] = df[col].fillna('unknown')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean all dataframes\n",
    "customers = clean_dataframe(customers)\n",
    "locations = clean_dataframe(locations)\n",
    "orders = clean_dataframe(orders)\n",
    "vendors = clean_dataframe(vendors)\n",
    "test_customers = clean_dataframe(test_customers)\n",
    "test_locations = clean_dataframe(test_locations)\n",
    "\n",
    "# Process customer age if DOB exists\n",
    "if 'dob' in customers.columns:\n",
    "    customers['dob'] = pd.to_datetime(customers['dob'], errors='coerce')\n",
    "    customers['age'] = 2025 - customers['dob'].dt.year\n",
    "    # Drop age if too many missing values\n",
    "    if customers['age'].isnull().mean() > 0.8:\n",
    "        customers = customers.drop(columns=['age', 'dob'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8174e7",
   "metadata": {},
   "source": [
    "TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86d3cd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual orders (positive samples): 80,142\n",
      "Positive rate: 0.0135\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get unique customer locations\n",
    "customer_locations = locations[['customer_id', 'location_number', 'location_type', 'latitude', 'longitude']].drop_duplicates()\n",
    "\n",
    "# Get unique vendors\n",
    "unique_vendors = orders['vendor_id'].unique()\n",
    "vendor_df = pd.DataFrame({'vendor_id': unique_vendors})\n",
    "\n",
    "# Create cartesian product (all possible combinations)\n",
    "customer_locations['key'] = 1\n",
    "vendor_df['key'] = 1\n",
    "all_combinations = customer_locations.merge(vendor_df, on='key').drop('key', axis=1)\n",
    "\n",
    "\n",
    "# positive classes\n",
    "actual_orders = orders[['customer_id', 'LOCATION_NUMBER', 'vendor_id']].copy()\n",
    "actual_orders.columns = ['customer_id', 'location_number', 'vendor_id']\n",
    "actual_orders['target'] = 1\n",
    "actual_orders = actual_orders.drop_duplicates()\n",
    "\n",
    "print(f\"Actual orders (positive samples): {len(actual_orders):,}\")\n",
    "\n",
    "# Merge dataset\n",
    "training_data = all_combinations.merge(\n",
    "    actual_orders[['customer_id', 'location_number', 'vendor_id', 'target']], \n",
    "    on=['customer_id', 'location_number', 'vendor_id'], \n",
    "    how='left'\n",
    ")\n",
    "training_data['target'] = training_data['target'].fillna(0).astype(int)\n",
    "\n",
    "print(f\"Positive rate: {training_data['target'].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be77cb7",
   "metadata": {},
   "source": [
    "Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da13116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training data: 240,426\n",
      "Class distr:\n",
      "target\n",
      "0    160284\n",
      "1     80142\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate positive and negative samples\n",
    "positive_samples = training_data[training_data['target'] == 1]\n",
    "negative_samples = training_data[training_data['target'] == 0]\n",
    "\n",
    "# Sample negatives: use 2x positive samples for better balance\n",
    "n_negative_samples = min(len(positive_samples) * 2, len(negative_samples))\n",
    "negative_samples_sampled = negative_samples.sample(n=n_negative_samples, random_state=42)\n",
    "\n",
    "# Combine and shuffle\n",
    "balanced_training = pd.concat([positive_samples, negative_samples_sampled])\n",
    "balanced_training = balanced_training.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Final training data: {len(balanced_training):,}\")\n",
    "print(f\"Class distr:\\n{balanced_training['target'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566092ce",
   "metadata": {},
   "source": [
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47305249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add customer features\n",
    "balanced_training = balanced_training.merge(\n",
    "    customers[['customer_id', 'gender', 'status', 'verified', 'language']], \n",
    "    on='customer_id', how='left'\n",
    ")\n",
    "\n",
    "# Add vendor features\n",
    "vendor_features = ['id', 'latitude', 'longitude', 'vendor_category_en', 'delivery_charge','serving_distance', 'vendor_rating', 'discount_percentage']\n",
    "available_vendor_features = [col for col in vendor_features if col in vendors.columns]\n",
    "balanced_training = balanced_training.merge(\n",
    "    vendors[available_vendor_features], \n",
    "    left_on='vendor_id', right_on='id', how='left', suffixes=('_customer', '_vendor')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate distance between customer and vendor (if coordinates available)\n",
    "if 'latitude_customer' in balanced_training.columns and 'latitude_vendor' in balanced_training.columns:\n",
    "    def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate distance between two points on Earth\"\"\"\n",
    "        R = 6371  # Earth radius in km\n",
    "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        return R * c\n",
    "    \n",
    "    balanced_training['customer_vendor_distance'] = haversine_distance(\n",
    "        balanced_training['latitude_customer'].fillna(0),\n",
    "        balanced_training['longitude_customer'].fillna(0),\n",
    "        balanced_training['latitude_vendor'].fillna(0),\n",
    "        balanced_training['longitude_vendor'].fillna(0)\n",
    "    )\n",
    "\n",
    "# Handle missing values in new features\n",
    "balanced_training = clean_dataframe(balanced_training, threshold=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fafff6",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4d5c532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['location_number', 'latitude_customer', 'longitude_customer', 'vendor_id', 'delivery_charge', 'serving_distance', 'discount_percentage', 'gender', 'location_type', 'vendor_category_en', 'customer_vendor_distance']\n"
     ]
    }
   ],
   "source": [
    "# Select relevant features for modeling\n",
    "feature_columns = [\n",
    "    'location_number', 'latitude_customer', 'longitude_customer',\n",
    "    'vendor_id', 'delivery_charge', 'serving_distance', 'discount_percentage'\n",
    "]\n",
    "\n",
    "# Add categorical features if they exist\n",
    "categorical_features = ['gender', 'location_type', 'vendor_category_en']\n",
    "for feat in categorical_features:\n",
    "    if feat in balanced_training.columns:\n",
    "        feature_columns.append(feat)\n",
    "\n",
    "# Add distance if calculated\n",
    "if 'customer_vendor_distance' in balanced_training.columns:\n",
    "    feature_columns.append('customer_vendor_distance')\n",
    "\n",
    "# Keep only available features\n",
    "feature_columns = [col for col in feature_columns if col in balanced_training.columns]\n",
    "\n",
    "print(f\"Selected features: {feature_columns}\")\n",
    "\n",
    "# Prepare feature matrix\n",
    "X = balanced_training[feature_columns].copy()\n",
    "y = balanced_training['target']\n",
    "\n",
    "# Handle categorical variables\n",
    "label_encoders = {}\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Fill any remaining missing values\n",
    "X = X.fillna(X.median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0444055e",
   "metadata": {},
   "source": [
    "TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba95bb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.8051\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86     32149\n",
      "           1       0.75      0.63      0.68     16056\n",
      "\n",
      "    accuracy                           0.81     48205\n",
      "   macro avg       0.79      0.76      0.77     48205\n",
      "weighted avg       0.80      0.81      0.80     48205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train XGBoost model\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"\\nValidation Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63cabc",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac739d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test combinations\n",
    "test_locations_clean = test_locations[['customer_id', 'location_number', 'location_type', 'latitude', 'longitude']].drop_duplicates()\n",
    "\n",
    "test_locations_clean['key'] = 1\n",
    "test_combinations = test_locations_clean.merge(vendor_df, on='key').drop('key', axis=1)\n",
    "\n",
    "# Add customer features\n",
    "test_combinations = test_combinations.merge(\n",
    "    test_customers[['customer_id', 'gender', 'status', 'verified', 'language']], \n",
    "    on='customer_id', how='left'\n",
    ")\n",
    "\n",
    "# Add vendor features\n",
    "test_combinations = test_combinations.merge(\n",
    "    vendors[available_vendor_features], \n",
    "    left_on='vendor_id', right_on='id', how='left', suffixes=('_customer', '_vendor')\n",
    ")\n",
    "\n",
    "# Calculate distance if possible\n",
    "if 'latitude_customer' in test_combinations.columns and 'latitude_vendor' in test_combinations.columns:\n",
    "    test_combinations['customer_vendor_distance'] = haversine_distance(\n",
    "        test_combinations['latitude_customer'].fillna(0),\n",
    "        test_combinations['longitude_customer'].fillna(0),\n",
    "        test_combinations['latitude_vendor'].fillna(0),\n",
    "        test_combinations['longitude_vendor'].fillna(0)\n",
    "    )\n",
    "\n",
    "# Clean test data\n",
    "test_combinations = clean_dataframe(test_combinations, threshold=0.7)\n",
    "\n",
    "# Prepare test features\n",
    "X_test = test_combinations[feature_columns].copy()\n",
    "\n",
    "# Apply same encodings\n",
    "for col in categorical_cols:\n",
    "    if col in X_test.columns and col in label_encoders:\n",
    "        # Handle unseen categories\n",
    "        X_test[col] = X_test[col].astype(str)\n",
    "        mask = X_test[col].isin(label_encoders[col].classes_)\n",
    "        X_test.loc[~mask, col] = label_encoders[col].classes_[0]  # Use first class for unseen\n",
    "        X_test[col] = label_encoders[col].transform(X_test[col])\n",
    "\n",
    "# Fill missing values\n",
    "X_test = X_test.fillna(X_test.median())\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = model.predict(X_test)\n",
    "test_probabilities = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b522a7",
   "metadata": {},
   "source": [
    "SUBMISSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4160732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model training and prediction complete!\n"
     ]
    }
   ],
   "source": [
    "# Add predictions to test combinations\n",
    "test_combinations['target'] = test_predictions\n",
    "test_combinations['probability'] = test_probabilities\n",
    "\n",
    "# Create submission format\n",
    "test_combinations['CID X LOC_NUM X VENDOR'] = (\n",
    "    test_combinations['customer_id'] + ' X ' + \n",
    "    test_combinations['location_number'].astype(str) + ' X ' + \n",
    "    test_combinations['vendor_id'].astype(str)\n",
    ")\n",
    "\n",
    "# Full submission\n",
    "submission_full = test_combinations[['CID X LOC_NUM X VENDOR', 'target']]\n",
    "submission_full.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nModel training and prediction complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
